{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp philander2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philander 2014\n",
    "\n",
    "> Full replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how gamba can be used to reproduce findings from Philander's 2014 study on data mining methods for detecting high-risk gamblers.\n",
    "\n",
    "- [Data Download (thetransparencyproject.org)](http://www.thetransparencyproject.org/download_index.php)\n",
    "- [Data Description]()\n",
    "- [Original Paper](https://www.tandfonline.com/doi/abs/10.1080/14459795.2013.841721)\n",
    "\n",
    "It uses data available through the transaparency project above, and performs eight distinct supervised machine learning techniques.\n",
    "\n",
    "**Note:** given the high dimensionality of this data (17), the sample size (530) doesn't meet the [rule of thumb](https://youtu.be/Dc0sr0kdBVI?t=3414) that 10x17 (or 1700) observations are required for learning to be generalisable. This means that the ouputs of the methods below may change drastically upon repeated executions, and comparison to the original may not be meaningful. That considered, this notebook shows you how to do this kind of analysis using identical methods.\n",
    "\n",
    "To begin, import gamba as usual;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gamba as gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530 players loaded\n"
     ]
    }
   ],
   "source": [
    "measures_table = gb.data.prepare_philander_data('AnalyticDataSet_HighRisk.txt', loud=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regressions\n",
    "The machine learning module has wrappers for two logistic regression functions which can be used here. As with other machine learning methods in the gamba library, they return both the actual test labels and the predicted labels so that performance metrics can be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: throughout this page the naming convention is an abbreviated version of the name for the test labels used, and the same name with a 'p' on the end to denote the predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: you'll also notice a train_test_split parameter of 0.696 as a parameter to all of the methods, this is just to make sure that exactly the same train test split happens as in the paper (it defaults to 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r, log_rp = gb.machine_learning.logistic_regression(measures_table, 'self_exclude', train_test_split=0.696)\n",
    "lasso_l, lasso_lp = gb.machine_learning.lasso_logistic_regression(measures_table, 'self_exclude', train_test_split=0.696)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "The following cell uses the [Keras](https://keras.io) library to create and train some neural networks as described in the study. The original study uses the R [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf#Rfn.optim) and [caret](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) packages, [this stackoverflow post](https://stackoverflow.com/questions/42417948/how-to-use-size-and-decay-in-nnet) was helpful in understanding the original parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Framing the self_exclude label (0 or 1) as a regression problem means creating a neural network which returns a continuous label. The classification version of the neural network used in the original analysis uses an identical network topology but passes two strings as values instead of a 1 or 0. This should in theory have no substantial difference on the performance of the network (given the sample size and identical architectures).\n",
    "\n",
    "By contrast, the gamba library's neural network methods have subtly different topologies for classification and regression as described in [Deep Learning with Python](http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf), which are used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_c, nn_cp = gb.machine_learning.neural_network_classification(measures_table, 'self_exclude', train_test_split=0.696)\n",
    "nn_r, nn_rp = gb.machine_learning.neural_network_regression(measures_table, 'self_exclude', train_test_split=0.696)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "The following cell uses [scikit-learn's SVM](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm) methods to create and trains some SVM's. The original paper uses Dimitriadou et al's [implementations in R described here](https://www.researchgate.net/profile/Friedrich_Leisch/publication/221678005_E1071_Misc_Functions_of_the_Department_of_Statistics_E1071_TU_Wien/links/547305880cf24bc8ea19ad1d/E1071-Misc-Functions-of-the-Department-of-Statistics-E1071-TU-Wien.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_e, svm_ep = gb.machine_learning.svm_eps_regression(measures_table, 'self_exclude', train_test_split=0.696)\n",
    "svm_c, svm_cp = gb.machine_learning.svm_c_classification(measures_table, 'self_exclude', train_test_split=0.696)\n",
    "svm_o, svm_op = gb.machine_learning.svm_one_classification(measures_table, 'self_exclude', train_test_split=0.696)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "This section implements [scikit-learn's ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html#forest) to create random forests for classification and regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_r, rf_rp = gb.machine_learning.rf_regression(measures_table, 'self_exclude', train_test_split=0.696)\n",
    "rf_c, rf_cp = gb.machine_learning.rf_classification(measures_table, 'self_exclude', train_test_split=0.696)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Methods Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets present the performance of each of the machine learning techniques using a number of metrics. Not all of the metrics apply to all of the methods, but it's a good way to see roughly how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>auc</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Logistic Regression</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.533</td>\n",
       "      <td>3.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN Regression</th>\n",
       "      <td>0.431</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN Classification</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM eps-Regression</th>\n",
       "      <td>0.020</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.505</td>\n",
       "      <td>2.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM c-Classification</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM one-Classification</th>\n",
       "      <td>0.462</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF Regression</th>\n",
       "      <td>0.373</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.564</td>\n",
       "      <td>1.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF Classification</th>\n",
       "      <td>0.241</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.555</td>\n",
       "      <td>2.106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           sensitivity  specificity  accuracy  precision  \\\n",
       "Logistic Regression              0.080        0.901     0.646      0.267   \n",
       "Lasso Logistic Regression        0.094        0.972     0.683      0.625   \n",
       "NN Regression                    0.431        0.545     0.509      0.306   \n",
       "NN Classification                0.000        1.000     0.640        NaN   \n",
       "SVM eps-Regression               0.020        0.991     0.683      0.500   \n",
       "SVM c-Classification             0.000        1.000     0.646        NaN   \n",
       "SVM one-Classification           0.462        0.505     0.491      0.308   \n",
       "RF Regression                    0.373        0.755     0.634      0.413   \n",
       "RF Classification                0.241        0.869     0.658      0.481   \n",
       "\n",
       "                             auc  odds_ratio  \n",
       "Logistic Regression        0.490       0.791  \n",
       "Lasso Logistic Regression  0.533       3.646  \n",
       "NN Regression              0.488       0.910  \n",
       "NN Classification          0.500       0.000  \n",
       "SVM eps-Regression         0.505       2.180  \n",
       "SVM c-Classification       0.500       0.000  \n",
       "SVM one-Classification     0.483       0.873  \n",
       "RF Regression              0.564       1.825  \n",
       "RF Classification          0.555       2.106  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_results = [\n",
    "    gb.machine_learning.compute_performance('Logistic Regression', log_r, log_rp),\n",
    "    gb.machine_learning.compute_performance('Lasso Logistic Regression', lasso_l, lasso_lp),\n",
    "    gb.machine_learning.compute_performance('NN Regression', nn_r, nn_rp),\n",
    "    gb.machine_learning.compute_performance('NN Classification', nn_c, nn_cp),\n",
    "    gb.machine_learning.compute_performance('SVM eps-Regression', svm_e, svm_ep),\n",
    "    gb.machine_learning.compute_performance('SVM c-Classification', svm_c, svm_cp),\n",
    "    gb.machine_learning.compute_performance('SVM one-Classification', svm_o, svm_op),\n",
    "    gb.machine_learning.compute_performance('RF Regression', rf_r, rf_rp),\n",
    "    gb.machine_learning.compute_performance('RF Classification', rf_c, rf_cp)\n",
    "]\n",
    "\n",
    "all_results_df = gb.concat(all_results)\n",
    "display(all_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
