{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp machine_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "> Apply simple machine learning algorithms to a measures table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gamba library contains several wrapper methods for commonly applied machine learning methods. Whilst it would be impossible to cover every machine learning method used in existing literature, the gamba library aims to provide a number of examples so that if you have a particular method in mind, some variant or similar implementation is available for you to extend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Clustering techniques have been popular in early literature, but continue to be applied across a number of problems. The gamba library contains wrapper methods around the scikit-learn library, which has implementations of a number of clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "#export\n",
    "def k_means(measures_table, clusters=4, data_only=False, plot=False, loud=False):\n",
    "    \"Applies the k-means clustering algorithm to a measures table. The resulting clustering is then reported in terms of its inertia (sum of squared distances of samples to their closest cluster center) and its silhouette score (how distinct clusters are within the sample [see the skikit learn docs for details]). The measures passed as the first parameter can be returned with an added column reporting the cluster each player belongs to using the data_only parameter.\"\n",
    "\n",
    "    # get variable names from the behavioural measures\n",
    "    variables = list(measures_table.columns)[1:]\n",
    "\n",
    "    Kmean = KMeans(n_clusters=clusters)\n",
    "\n",
    "    data = np.array(measures_table[variables].values)\n",
    "    Kmean.fit(data)\n",
    "\n",
    "    silhouette = metrics.silhouette_score(data, Kmean.labels_, metric=\"euclidean\")\n",
    "\n",
    "    cluster_centers = Kmean.cluster_centers_\n",
    "\n",
    "    clustered_data = measures_table.copy()\n",
    "    clustered_data[\"cluster\"] = Kmean.labels_\n",
    "\n",
    "    if loud:\n",
    "        print(\"variables:\", variables)\n",
    "        print(\"centers:\", Kmean.cluster_centers_)\n",
    "        print(\"inertia:\", Kmean.inertia_)\n",
    "        print(\"silhouette:\", silhouette)\n",
    "\n",
    "    if plot:\n",
    "        bars = []\n",
    "        heights = []\n",
    "        for label in set(sorted(Kmean.labels_)):\n",
    "            bars.append(label)\n",
    "            heights.append(list(Kmean.labels_).count(label))\n",
    "\n",
    "        colors = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\"]\n",
    "        plt.bar(bars, heights, color=colors[: len(bars)])\n",
    "        plt.title(\n",
    "            \"\\nClusters: \"\n",
    "            + str(len(bars))\n",
    "            + \"\\nInertia: \"\n",
    "            + str(round(Kmean.inertia_))\n",
    "            + \"\\nIterations: \"  # Kmean.inertia_ is the sum of squared distances of samples to their closest cluster center\n",
    "            + str(Kmean.n_iter_),\n",
    "            x=1.01,\n",
    "            y=0.5,\n",
    "            ha=\"left\",\n",
    "        )\n",
    "        plt.xlabel(\"Cluster ID\")\n",
    "        plt.ylabel(\"Number of Members\")\n",
    "        plt.show()\n",
    "\n",
    "    if data_only:\n",
    "        return clustered_data\n",
    "\n",
    "    return clustered_data, Kmean.inertia_, silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def k_means_range(measures_table, min_clusters=2, max_clusters=13):\n",
    "    \"Computes the k_means calculation above across a range of cluster counts, returning their goodness of fit measures (inertia and silhouette).\"\n",
    "\n",
    "    # print('calculating k means in range', min_clusters, max_clusters)\n",
    "\n",
    "    inertias = []\n",
    "    silhouettes = []\n",
    "\n",
    "    for x in range(min_clusters, max_clusters + 1):\n",
    "        k_means_result = k_means(measures_table, clusters=x)\n",
    "        inertias.append(k_means_result[1])\n",
    "        silhouettes.append(k_means_result[2])\n",
    "\n",
    "    return inertias, silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def k_means_ensemble(measures_table, ensemble_size=100, min_clusters=2, max_clusters=13):\n",
    "    \"Computes the k_means clustering algorithm across a range of cluster counts, a number of times. This is useful for determining clusters in a more robust way but can be slow on large data sets.\"\n",
    "\n",
    "    all_inertias = []\n",
    "    all_silhouettes = []\n",
    "\n",
    "    # call the k_means_range function n times, storing scores in the above arrays\n",
    "    for x in range(ensemble_size):\n",
    "        k_means_range_result = k_means_range(\n",
    "            measures_table, min_clusters=min_clusters, max_clusters=max_clusters\n",
    "        )\n",
    "        all_inertias.append(k_means_range_result[0])\n",
    "        all_silhouettes.append(k_means_range_result[1])\n",
    "\n",
    "    # now average each of the elements in the score lists\n",
    "    ensemble_inertias = []\n",
    "    ensemble_silhouettes = []\n",
    "    for cluster_num in range(len(all_inertias[0])):\n",
    "        inertia_scores = [all_inertias[x][cluster_num] for x in range(ensemble_size)]\n",
    "        ensemble_inertias.append(statistics.mean(inertia_scores))\n",
    "\n",
    "        silhouette_scores = [\n",
    "            all_silhouettes[x][cluster_num] for x in range(ensemble_size)\n",
    "        ]\n",
    "        ensemble_silhouettes.append(statistics.mean(silhouette_scores))\n",
    "\n",
    "    return ensemble_inertias, ensemble_silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#export\n",
    "def agglomerative_cluster(measures_table, distance_threshold=0, n_clusters=None):\n",
    "    \"Performs sklearn's agglomerative clustering algorithm on a dataframe of behavioural measures. See their documentation for details. Note: Either the distance threshold or the n_cluster parameter must be None.\"\n",
    "    \n",
    "    variables = list(measures_table.columns)[1:]\n",
    "    X = measures_table[variables].values\n",
    "\n",
    "    model = AgglomerativeClustering(\n",
    "        distance_threshold=distance_threshold, n_clusters=n_clusters\n",
    "    )\n",
    "    model = model.fit(X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pandas as pd\n",
    "def describe_clusters(clustered_measures_table, cluster_col=\"cluster\"):\n",
    "    \"Describes cluster centroids (mean values of each measure) for each cluster in a clustered measures table.\"\n",
    "    \n",
    "    unique_clusters = list(set(clustered_measures_table[\"cluster\"].values))\n",
    "\n",
    "    descriptive_table = pd.DataFrame()\n",
    "    descriptive_table[\"cluster_centroid\"] = clustered_measures_table.columns[1:]\n",
    "\n",
    "    for value in unique_clusters:\n",
    "        members = clustered_measures_table[\n",
    "            clustered_measures_table[cluster_col] == value\n",
    "        ]\n",
    "        centroid = [members[col].mean() for col in members.columns[1:]]\n",
    "        descriptive_table[\"n=\" + str(len(members))] = centroid\n",
    "\n",
    "    descriptive_table.set_index(\"cluster_centroid\", inplace=True)\n",
    "    return descriptive_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Neural networks are a powerful way to learn high dimensional patterns in transaction data sets. The gamba library contains limited neural network capabilities, but provide fully annotated methods so you can copy them into your own workflows and extend them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import gamba.measures as measures\n",
    "\n",
    "def neural_network_classification(measures_table, label, train_test_split=0.7):\n",
    "    \"A basic example of a classification network using a single sigmoid-activated output node. The output is the probability of the player belonging to the value of the label. As with the `neural_network_regression` this network closely follows examples in [Deep Learning with Python](http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf) which is a great resource for extending this example.\"\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "    \n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id',label], axis=1)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(17, activation = 'relu', input_dim = 17))\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    model.add(Dense(units = 1, activation='sigmoid')) # this is the 'classification' part\n",
    "\n",
    "    model.compile(optimizer = 'adam', \n",
    "                  loss = 'mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, train_labels, \n",
    "                        batch_size = 20, epochs = 100,\n",
    "                        validation_data=(test_data, test_labels),\n",
    "                        verbose=False)\n",
    "\n",
    "    # now make a prediction and clip the values to 0 or 1 as in the original code\n",
    "    raw_prediction = model.predict(test_data)\n",
    "    predicted_labels = [value[0] for value in np.where(raw_prediction >= 0.5, 1, 0)]\n",
    "    \n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import gamba.measures as measures\n",
    "def neural_network_regression(measures_table, label, train_test_split=0.7):\n",
    "    \"A very simple neural network for regression using two hidden layers and one output node with a linear activation function. This method is very similar to the example in [Deep Learning with Python](http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf), and can be used as a template for more complex network topologies.\"\n",
    "    \n",
    "    # note that the metric of accuracy doesn't apply for regression problems (mean absolute error is better)\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "    \n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id',label], axis=1)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(17, activation = 'relu', input_dim = 17))\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "    model.add(Dense(units = 1)) # see page 86 of Deep Learning with Python by Chollet for an explanation\n",
    "\n",
    "    model.compile(optimizer = 'adam', \n",
    "                  loss = 'mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, train_labels, \n",
    "                        batch_size = 20, epochs = 100,\n",
    "                        validation_data=(test_data, test_labels),\n",
    "                        verbose=False)\n",
    "\n",
    "    # now make a prediction and clip the values to 0 or 1 as in the original code\n",
    "    raw_prediction = model.predict(test_data)\n",
    "    predicted_labels = [value[0] for value in np.where(raw_prediction >= 0.5, 1, 0)]\n",
    "    \n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression techniques can map simple relationships between variables, but typically become untenable in more complex applications. As with the clustering techniques, the gamba library contains wrappers around the sklearn library's logistic regression functions, offering simple methods to run analyses without having to interact with the sklearn library directly (although you should for advanced techniques!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import gamba.measures as measures\n",
    "#export\n",
    "def logistic_regression(measures_table, label, train_test_split=0.7):\n",
    "    \"Performs a logistic regression using the [statsmodels library](https://www.statsmodels.org/stable/index.html), returning the predicted labels rounded to the nearest integer. Note: this method is currently hard-coded to only function on Philander 2014's data set. Abstracted logistic regression function coming soon.\"\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "\n",
    "    # defines the R style formula to fit\n",
    "    formula = str(label) + \" ~ gender+age+total_wagered+num_bets+frequency+duration+bets_per_day+net_loss+intensity+variability+frequency_1m+trajectory+z_intensity+z_variability+z_frequency+z_trajectory\"\n",
    "    model = sm.formula.glm(formula=formula, family=sm.families.Binomial(), data=train_measures)\n",
    "\n",
    "    # this is where the stepwise bit could happen - see original code\n",
    "    fit_model = model.fit()\n",
    "\n",
    "    raw_prediction = fit_model.predict(test_measures)\n",
    "    predicted_labels = [value for value in np.where(raw_prediction >= 0.5, 1, 0)]\n",
    "\n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import gamba.measures as measures\n",
    "def lasso_logistic_regression(measures_table, label, train_test_split=0.7):\n",
    "    \"Performs a 'lasso' (optimizes a least-square problem with L1 penalty) logistic regression using [sklearn's linear_model](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model). This [stackoverflow post](https://stackoverflow.com/questions/41639557/how-to-perform-logistic-lasso-in-python>) contains a useful discussion on this type of function-estimation regression pair.\"\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM's are another type of machine learning algorithm but have seen limited use in existing work, but are included as templates to see how they could be used as part of an analysis using the gamba library. The three SVM methods implemented include those used in Philander's 2014 study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import gamba.measures as measures\n",
    "def svm_eps_regression(measures_table, label, train_test_split=0.7):\n",
    "    \"Creates and trains a support vector machine for epsilon-support vector regression using the sklearn library's implementation.\"\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = svm.SVR(kernel='rbf')\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    # convert probabilities to binary labels for comparison\n",
    "    regression_cutoff = 0.5\n",
    "    predicted_labels = np.where(predicted_labels < regression_cutoff, 0, 1)\n",
    "\n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import gamba.measures as measures\n",
    "def svm_c_classification(measures_table, label, train_test_split=0.7):\n",
    "    \"Creates and trains a support vector machine for classification using the sklearn library's implementation.\"\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = svm.SVC(kernel='rbf')\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import gamba.measures as measures\n",
    "def svm_one_classification(measures_table, label, train_test_split=0.7):\n",
    "    \"Creates and trains a support vector machine for one-class classification using the sklearn library's implementation.\"\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = svm.OneClassSVM(kernel='rbf')\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    # need to add a correction step for the labels here as OneClassSVM returns -1 for outliers and 1 for inliers\n",
    "    predicted_labels = np.where(predicted_labels < 0, 1, 0)\n",
    "\n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are collections of decision trees which are applied en-mass to classification and regression problems. The gamba library implements both techniques in a very basic form, which follow a similar structure to other machine learning methods in the library. As with many other methods on this page, the gamba library simply contains wrappers for the scikit-learn library, which should be used directly for more advanced analyses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gamba.measures as measures\n",
    "#export\n",
    "def rf_regression(measures_table, label, train_test_split=0.7):\n",
    "    \"Creates and fits a random forest regressor using [sklearn's ensemble module](https://scikit-learn.org/stable/modules/ensemble.html#forest), returning the predicted labels rounded to the nearest integer.\"\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    # convert probabilities to binary labels for comparison\n",
    "    regression_cutoff = 0.5\n",
    "    predicted_labels = np.where(predicted_labels < regression_cutoff, 0, 1)\n",
    "\n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import gamba.measures as measures\n",
    "def rf_classification(measures_table, label, train_test_split=0.7):\n",
    "    \"Creates a random forest for classification also using the sklearn library's implementation.\"\n",
    "    \n",
    "    train_measures, test_measures = measures.split_measures_table(measures_table, frac=train_test_split)\n",
    "    test_labels = test_measures[label]\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    return test_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance measures of many machine learning methods can be derived from the values they predict and the actual values of a test dataset. The `compute_performance` function below is useful for quick estimates of performance, but you should refer to the measures appropriate to the specific method you're implementing for advanced analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#export\n",
    "def compute_performance(method_name, actual, predicted):\n",
    "    \"Computes performance metrics including sensitivity, specificity, accuracy, confusion matrix values, odds ratio, and area under curve, for a given classification/regression using its actual and predicted values.\"\n",
    "    \n",
    "    # resources:\n",
    "    # describes odds ratio and precision equations\n",
    "    # https://cran.r-project.org/web/packages/ROCR/ROCR.pdf\n",
    "    # describes sklearn's confusion matrix\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn-metrics-confusion-matrix\n",
    "\n",
    "    result = metrics.classification_report(actual, y_pred=predicted, output_dict=True)\n",
    "    sensitivity = result['1']['recall']\n",
    "    specificity = result['0']['recall']\n",
    "    accuracy = result['accuracy']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual, predicted)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "    # odds ratio is (tp x tn)/(fp x fn)\n",
    "    odds_ratio = 0\n",
    "    if fp != 0 and fn != 0:\n",
    "        odds_ratio = (tp * tn)/(fp * fn)\n",
    "\n",
    "    # precision is tp / (tp + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    metrics_df = pd.DataFrame()\n",
    "    metrics_df['sensitivity'] = [round(sensitivity, 3)]\n",
    "    metrics_df['specificity'] = [round(specificity, 3)]\n",
    "    metrics_df['accuracy'] = [round(accuracy, 3)]\n",
    "    metrics_df['precision'] = [round(precision, 3)]\n",
    "    metrics_df['auc'] = [round(auc, 3)]\n",
    "    metrics_df['odds_ratio'] = [round(odds_ratio, 3)]\n",
    "    metrics_df.index = [method_name]\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Some machine learning outputs favour plotting over raw tabular presentation. An example of this is a hierarchical clustering dendrogram, which is much more intuitive than a cluster membership report. The gamba library contains several methods for plotting the outputs of machine learning models;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.ndimage.filters as snf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_cluster_sizes(model):\n",
    "    \"Create a bar chart using a previously computed clustering model. Each bar represents a single cluster, with the height of the bars representing the number of members (players) in each cluster.\"\n",
    "    \n",
    "    plt.figure()\n",
    "    cluster_ids = list(set(list(model.labels_)))\n",
    "    cluster_sizes = [list(model.labels_).count(x) for x in cluster_ids]\n",
    "    plt.bar(\n",
    "        cluster_ids,\n",
    "        cluster_sizes,\n",
    "        color=plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"],\n",
    "    )\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.xticks(range(len(cluster_ids)), cluster_ids)\n",
    "    plt.xlabel(\"Cluster ID\")\n",
    "    plt.ylabel(\"Number of Players per Cluster\")\n",
    "    plt.grid(axis=\"x\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_agglomeration_dendrogram(model, dt_cutoff=None, **kwargs):\n",
    "    \"Create a dendrogram visualising a heirarchical clustering method (agglomerative clustering). A horisontal line can be added using the dt_cutoff parameter to visualise the number of clusters at a given distance threshold.\"\n",
    "    \n",
    "    # Create linkage matrix and then plot the sch.dendrogram\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.title(\"Hierarchical Clustering dendrogram\")\n",
    "    sch.dendrogram(linkage_matrix, truncate_mode=\"level\", p=3)\n",
    "    if dt_cutoff != None:\n",
    "        plt.plot(list(plt.xlim()), [dt_cutoff, dt_cutoff], linestyle=\"--\", color=\"grey\")\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.ylabel(\"Distance threshold\")\n",
    "    plt.grid(False)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
