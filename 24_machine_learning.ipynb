{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp machine_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "> Apply simple machine learning algorithms to a measures table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gamba library contains several wrapper methods for commonly applied machine learning methods. Whilst it would be impossible to cover every machine learning method used in existing literature, the gamba library aims to provide a number of examples so that if you have a particular method in mind, some variant or similar implementation is available for you to extend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Clustering techniques have been popular in early literature, but continue to be applied across a number of problems. The gamba library contains wrapper methods around the scikit-learn library, which has implementations of a number of clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#export\n",
    "def k_means(measures_table, clusters=4, data_only=False, plot=False, loud=False):\n",
    "    \"Applies the k-means clustering algorithm to a measures table. The resulting clustering is then reported in terms of its inertia (sum of squared distances of samples to their closest cluster center) and its silhouette score (how distinct clusters are within the sample [see the skikit learn docs for details]). The measures passed as the first parameter can be returned with an added column reporting the cluster each player belongs to using the data_only parameter.\"\n",
    "\n",
    "    # get variable names from the behavioural measures\n",
    "    variables = list(measures_table.columns)[1:]\n",
    "\n",
    "    Kmean = KMeans(n_clusters=clusters)\n",
    "\n",
    "    data = np.array(measures_table[variables].values)\n",
    "    Kmean.fit(data)\n",
    "\n",
    "    silhouette = metrics.silhouette_score(data, Kmean.labels_, metric=\"euclidean\")\n",
    "\n",
    "    cluster_centers = Kmean.cluster_centers_\n",
    "\n",
    "    clustered_data = measures_table.copy()\n",
    "    clustered_data[\"cluster\"] = Kmean.labels_\n",
    "\n",
    "    if loud:\n",
    "        print(\"variables:\", variables)\n",
    "        print(\"centers:\", Kmean.cluster_centers_)\n",
    "        print(\"inertia:\", Kmean.inertia_)\n",
    "        print(\"silhouette:\", silhouette)\n",
    "\n",
    "    if plot:\n",
    "        bars = []\n",
    "        heights = []\n",
    "        for label in set(sorted(Kmean.labels_)):\n",
    "            bars.append(label)\n",
    "            heights.append(list(Kmean.labels_).count(label))\n",
    "\n",
    "        colors = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\"]\n",
    "        plt.bar(bars, heights, color=colors[: len(bars)])\n",
    "        plt.title(\n",
    "            \"\\nClusters: \"\n",
    "            + str(len(bars))\n",
    "            + \"\\nInertia: \"\n",
    "            + str(round(Kmean.inertia_))\n",
    "            + \"\\nIterations: \"  # Kmean.inertia_ is the sum of squared distances of samples to their closest cluster center\n",
    "            + str(Kmean.n_iter_),\n",
    "            x=1.01,\n",
    "            y=0.5,\n",
    "            ha=\"left\",\n",
    "        )\n",
    "        plt.xlabel(\"Cluster ID\")\n",
    "        plt.ylabel(\"Number of Members\")\n",
    "        plt.show()\n",
    "\n",
    "    if data_only:\n",
    "        return clustered_data\n",
    "\n",
    "    return clustered_data, Kmean.inertia_, silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def k_means_range(measures_table, min_clusters=2, max_clusters=13):\n",
    "    \"Computes the k_means calculation above across a range of cluster counts, returning their goodness of fit measures (inertia and silhouette).\"\n",
    "\n",
    "    # print('calculating k means in range', min_clusters, max_clusters)\n",
    "\n",
    "    inertias = []\n",
    "    silhouettes = []\n",
    "\n",
    "    for x in range(min_clusters, max_clusters + 1):\n",
    "        k_means_result = k_means(measures_table, clusters=x)\n",
    "        inertias.append(k_means_result[1])\n",
    "        silhouettes.append(k_means_result[2])\n",
    "\n",
    "    return inertias, silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def k_means_ensemble(measures_table, ensemble_size=100, min_clusters=2, max_clusters=13):\n",
    "    \"Computes the k_means clustering algorithm across a range of cluster counts, a number of times. This is useful for determining clusters in a more robust way but can be slow on large data sets.\"\n",
    "\n",
    "    all_inertias = []\n",
    "    all_silhouettes = []\n",
    "\n",
    "    # call the k_means_range function n times, storing scores in the above arrays\n",
    "    for x in range(ensemble_size):\n",
    "        k_means_range_result = k_means_range(\n",
    "            measures_table, min_clusters=min_clusters, max_clusters=max_clusters\n",
    "        )\n",
    "        all_inertias.append(k_means_range_result[0])\n",
    "        all_silhouettes.append(k_means_range_result[1])\n",
    "\n",
    "    # now average each of the elements in the score lists\n",
    "    ensemble_inertias = []\n",
    "    ensemble_silhouettes = []\n",
    "    for cluster_num in range(len(all_inertias[0])):\n",
    "        inertia_scores = [all_inertias[x][cluster_num] for x in range(ensemble_size)]\n",
    "        ensemble_inertias.append(statistics.mean(inertia_scores))\n",
    "\n",
    "        silhouette_scores = [\n",
    "            all_silhouettes[x][cluster_num] for x in range(ensemble_size)\n",
    "        ]\n",
    "        ensemble_silhouettes.append(statistics.mean(silhouette_scores))\n",
    "\n",
    "    return ensemble_inertias, ensemble_silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#export\n",
    "def agglomerative_cluster(measures_table, distance_threshold=0, n_clusters=None):\n",
    "    \"Performs sklearn's agglomerative clustering algorithm on a dataframe of behavioural measures. See their documentation for details. Note: Either the distance threshold or the n_cluster parameter must be None.\"\n",
    "    \n",
    "    variables = list(measures_table.columns)[1:]\n",
    "    X = measures_table[variables].values\n",
    "\n",
    "    model = AgglomerativeClustering(\n",
    "        distance_threshold=distance_threshold, n_clusters=n_clusters\n",
    "    )\n",
    "    model = model.fit(X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def describe_clusters(clustered_measures_table, cluster_col=\"cluster\"):\n",
    "    \"Describes cluster centroids (mean values of each measure) for each cluster in a clustered measures table.\"\n",
    "    \n",
    "    unique_clusters = list(set(clustered_measures_table[\"cluster\"].values))\n",
    "\n",
    "    descriptive_table = pd.DataFrame()\n",
    "    descriptive_table[\"cluster_centroid\"] = clustered_measures_table.columns[1:]\n",
    "\n",
    "    for value in unique_clusters:\n",
    "        members = clustered_measures_table[\n",
    "            clustered_measures_table[cluster_col] == value\n",
    "        ]\n",
    "        centroid = [members[col].mean() for col in members.columns[1:]]\n",
    "        descriptive_table[\"n=\" + str(len(members))] = centroid\n",
    "\n",
    "    descriptive_table.set_index(\"cluster_centroid\", inplace=True)\n",
    "    return descriptive_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Neural networks are a powerful way to learn high dimensional patterns in transaction data sets. The gamba library contains limited neural network capabilities, but provide fully annotated methods so you can copy them into your own workflows and extend them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "#export\n",
    "def logistic_regression(train_measures, test_measures, label):\n",
    "    \"Performs a logistic regression using the [statsmodels library](https://www.statsmodels.org/stable/index.html), returning the predicted labels rounded to the nearest integer. Note: this method is currently hard-coded to only function on Philander 2014's data set. Abstracted logistic regression function coming soon.\"\n",
    "\n",
    "\n",
    "    # defines the R style formula to fit\n",
    "    formula = str(label) + \" ~ gender+age+total_wagered+num_bets+frequency+duration+bets_per_day+net_loss+intensity+variability+frequency_1m+trajectory+z_intensity+z_variability+z_frequency+z_trajectory\"\n",
    "    model = sm.formula.glm(formula=formula, family=sm.families.Binomial(), data=train_measures)\n",
    "\n",
    "    # this is where the stepwise bit could happen - see original code\n",
    "    fit_model = model.fit()\n",
    "\n",
    "    raw_prediction = fit_model.predict(test_measures)\n",
    "    predicted_labels = [value for value in np.where(raw_prediction >= 0.5, 1, 0)]\n",
    "\n",
    "    #print(fit_model.summary())\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lasso_logistic_regression(train_measures, test_measures, label):\n",
    "    \"Performs a 'lasso' (optimizes a least-square problem with L1 penalty) logistic regression using [sklearn's linear_model](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model). This [stackoverflow post](https://stackoverflow.com/questions/41639557/how-to-perform-logistic-lasso-in-python>) contains a useful discussion on this type of function-estimation regression pair.\"\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def svm_eps_regression(train_measures, test_measures, label):\n",
    "    \"Creates and trains a support vector machine for epsilon-support vector regression using the sklearn library's implementation.\"\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = svm.SVR(kernel='rbf')\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    # convert probabilities to binary labels for comparison\n",
    "    regression_cutoff = 0.5\n",
    "    predicted_labels = np.where(predicted_labels < regression_cutoff, 0, 1)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def svm_c_classification(train_measures, test_measures, label):\n",
    "    \"Creates and trains a support vector machine for classification using the sklearn library's implementation.\"\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = svm.SVC(kernel='rbf')\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def svm_one_classification(train_measures, test_measures, label):\n",
    "    \"Creates and trains a support vector machine for one-class classification using the sklearn library's implementation.\"\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = svm.OneClassSVM(kernel='rbf')\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    # need to add a correction step for the labels here as OneClassSVM returns -1 for outliers and 1 for inliers\n",
    "    predicted_labels = np.where(predicted_labels < 0, 1, 0)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#export\n",
    "def rf_regression(train_measures, test_measures, label):\n",
    "    \"Creates and fits a random forest regressor using [sklearn's ensemble module](https://scikit-learn.org/stable/modules/ensemble.html#forest), returning the predicted labels rounded to the nearest integer.\"\n",
    "\n",
    "    train_data = train_measures.drop(['player_id', label], axis=1)\n",
    "    train_labels = train_measures[label]\n",
    "    test_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    predicted_labels = model.predict(test_data)\n",
    "\n",
    "    # convert probabilities to binary labels for comparison\n",
    "    regression_cutoff = 0.5\n",
    "    predicted_labels = np.where(predicted_labels < regression_cutoff, 0, 1)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rf_classification(train_measures, test_measures, label):\n",
    "\t\"Creates a random forest for classification also using the sklearn library's \"\n",
    "\t\n",
    "\ttrain_data = train_measures.drop(['player_id', label], axis=1)\n",
    "\ttrain_labels = train_measures[label]\n",
    "\ttest_data = test_measures.drop(['player_id', label], axis=1)\n",
    "\t\n",
    "\tmodel = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "\tmodel.fit(train_data, train_labels)\n",
    "\n",
    "\tpredicted_labels = model.predict(test_data)\n",
    "\n",
    "\treturn predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#export\n",
    "def compute_performance(method_name, actual, predicted):\n",
    "    \"Computes performance metrics including sensitivity, specificity, accuracy, confusion matrix values, odds ratio, and area under curve, for a given classification/regression using its actual and predicted values.\"\n",
    "    \n",
    "    # resources:\n",
    "    # describes odds ratio and precision equations\n",
    "    # https://cran.r-project.org/web/packages/ROCR/ROCR.pdf\n",
    "    # describes sklearn's confusion matrix\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn-metrics-confusion-matrix\n",
    "\n",
    "    result = metrics.classification_report(actual, y_pred=predicted, output_dict=True)\n",
    "    sensitivity = result['1']['recall']\n",
    "    specificity = result['0']['recall']\n",
    "    accuracy = result['accuracy']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual, predicted)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
    "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "    # odds ratio is (tp x tn)/(fp x fn)\n",
    "    odds_ratio = 0\n",
    "    if fp != 0 and fn != 0:\n",
    "        odds_ratio = (tp * tn)/(fp * fn)\n",
    "\n",
    "    # precision is tp / (tp + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "\n",
    "    metrics_df = pd.DataFrame()\n",
    "    metrics_df['sensitivity'] = [round(sensitivity, 3)]\n",
    "    metrics_df['specificity'] = [round(specificity, 3)]\n",
    "    metrics_df['accuracy'] = [round(accuracy, 3)]\n",
    "    metrics_df['precision'] = [round(precision, 3)]\n",
    "    metrics_df['auc'] = [round(auc, 3)]\n",
    "    metrics_df['odds_ratio'] = [round(odds_ratio, 3)]\n",
    "    metrics_df.index = [method_name]\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Some machine learning outputs favour plotting over raw tabular presentation. An example of this is a hierarchical clustering dendrogram, which is much more intuitive than a cluster membership report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.ndimage.filters as snf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_cluster_sizes(model):\n",
    "    \"Create a bar chart using a previously computed clustering model. Each bar represents a single cluster, with the height of the bars representing the number of members (players) in each cluster.\"\n",
    "    \n",
    "    plt.figure()\n",
    "    cluster_ids = list(set(list(model.labels_)))\n",
    "    cluster_sizes = [list(model.labels_).count(x) for x in cluster_ids]\n",
    "    plt.bar(\n",
    "        cluster_ids,\n",
    "        cluster_sizes,\n",
    "        color=plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"],\n",
    "    )\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.xticks(range(len(cluster_ids)), cluster_ids)\n",
    "    plt.xlabel(\"Cluster ID\")\n",
    "    plt.ylabel(\"Number of Players per Cluster\")\n",
    "    plt.grid(axis=\"x\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_agglomeration_dendrogram(model, dt_cutoff=None, **kwargs):\n",
    "    \"Create a dendrogram visualising a heirarchical clustering method (agglomerative clustering). A horisontal line can be added using the dt_cutoff parameter to visualise the number of clusters at a given distance threshold.\"\n",
    "    \n",
    "    # Create linkage matrix and then plot the sch.dendrogram\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.title(\"Hierarchical Clustering dendrogram\")\n",
    "    sch.dendrogram(linkage_matrix, truncate_mode=\"level\", p=3)\n",
    "    if dt_cutoff != None:\n",
    "        plt.plot(list(plt.xlim()), [dt_cutoff, dt_cutoff], linestyle=\"--\", color=\"grey\")\n",
    "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "    plt.ylabel(\"Distance threshold\")\n",
    "    plt.grid(False)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
